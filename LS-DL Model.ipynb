{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad505c48",
   "metadata": {},
   "source": [
    "# Major Project - LSDL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9e9a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from matplotlib.backends.backend_pdf import PdfPages  \n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8985cac3",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24101b17",
   "metadata": {},
   "source": [
    "### Data Configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d680f23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_CONFIGS = {\n",
    "    \"Traffic\": {\n",
    "        \"filename\": \"PeMS\", \n",
    "        \"P\": 24, \"W\": 48, \"CK\": 6, \"horizons\": [3, 6, 12, 24], \"resample\": False\n",
    "    },\n",
    "    \"Solar\": {\n",
    "        \"filename\": \"solar_Alabama\", \n",
    "        \"P\": 144, \"W\": 36, \"CK\": 6, \"horizons\": [3, 6, 12, 24], \"resample\": False\n",
    "    },\n",
    "    \"Electricity\": {\n",
    "        \"filename\": \"LD2011_2014\", \n",
    "        \"P\": 24, \"W\": 48, \"CK\": 6, \"horizons\": [3, 6, 12, 24], \"resample\": \"1H\"\n",
    "    },\n",
    "    \"Exchange\": {\n",
    "        \"filename\": \"exchange\", \n",
    "        \"P\": 7, \"W\": 14, \"CK\": 6, \"horizons\": [3, 6, 12, 24], \"resample\": False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be5ff38",
   "metadata": {},
   "source": [
    "### Load and Process Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ce05da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_data(config_name):\n",
    "    cfg = DATASET_CONFIGS[config_name]\n",
    "    fname = cfg[\"filename\"]\n",
    "    print(f\"\\n[Loader] Reading {config_name} from '{fname}'...\")\n",
    "    try:\n",
    "        if config_name == \"Exchange\":\n",
    "            df = pd.read_csv(f\"{fname}.csv\", sep=',', decimal='.', header=0)\n",
    "        else:\n",
    "            try:\n",
    "                df = pd.read_csv(f\"datset\\{fname}\", header=0)\n",
    "            except FileNotFoundError:\n",
    "                df = pd.read_csv(f\"{fname}.csv\", header=0)\n",
    "                if config_name == 'exchange': print(df.head())\n",
    "\n",
    "        df_numeric = df.select_dtypes(include=[np.number])\n",
    "        \n",
    "        if cfg[\"resample\"] == \"1H\":\n",
    "            print(\"   -> Resampling to Hourly...\")\n",
    "            data_values = df_numeric.values\n",
    "            limit = (data_values.shape[0] // 4) * 4\n",
    "            data_values = data_values[:limit].reshape(-1, 4, data_values.shape[1]).mean(axis=1)\n",
    "            data_values = data_values[-26304:]\n",
    "            PAPER_CLIENT_COUNT = 321\n",
    "            if data_values.shape[1] > PAPER_CLIENT_COUNT:\n",
    "                print(f\"   -> Filtering Clients: Keeping top {PAPER_CLIENT_COUNT} active users...\")\n",
    "                client_activity = np.mean(data_values, axis=0)            \n",
    "                top_client_indices = np.argsort(client_activity)[-PAPER_CLIENT_COUNT:]\n",
    "                top_client_indices = np.sort(top_client_indices)\n",
    "                data_values = data_values[:, top_client_indices] \n",
    "                df_numeric = pd.DataFrame(data_values)\n",
    "\n",
    "        return df_numeric.values\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"   [Error] Could not load {fname}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7310afe4",
   "metadata": {},
   "source": [
    "### Creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e1ee6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data, window_size, period, horizon):\n",
    "\n",
    "    # Constructs X (Long-term) and Q (Short-term) inputs.\n",
    "\n",
    "    X_long, Q_short, Y = [], [], []\n",
    "    \n",
    "    # We need enough history for the long-term component.\n",
    "    # Assuming long-term looks back at 'period' intervals.\n",
    "    start_index = max(window_size, period * window_size) \n",
    "    \n",
    "    for i in range(start_index, len(data) - horizon):\n",
    "        # Short-term (Q): Immediate history [i-W : i]\n",
    "        short_term = data[i - window_size : i]\n",
    "        \n",
    "        # Long-term (X): Historical periodic data\n",
    "        # We sample the same time of day from previous days\n",
    "        indices = [i - (p * period) for p in range(window_size, 0, -1)]\n",
    "        long_term = data[indices]\n",
    "        \n",
    "        # Target (Y): The value at horizon\n",
    "        target = data[i + horizon - 1]\n",
    "        \n",
    "        X_long.append(long_term)\n",
    "        Q_short.append(short_term)\n",
    "        Y.append(target)\n",
    "        \n",
    "    return np.array(X_long), np.array(Q_short), np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a0e93a",
   "metadata": {},
   "source": [
    "## LS-DL Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0975e124",
   "metadata": {},
   "source": [
    "### General Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2156a80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7de384",
   "metadata": {},
   "source": [
    "### Model Architecture Definition:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220d51c1",
   "metadata": {},
   "source": [
    "### 1) LS-DL Model Architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e24dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSDL(nn.Module):\n",
    "    def __init__(self, num_variables, window_size, horizon=1):\n",
    "        super(LSDL, self).__init__()\n",
    "        \n",
    "        # --- Hyperparameters (Table 4) ---\n",
    "        self.conv_hidden = 200\n",
    "        self.rnn_hidden = 100\n",
    "        self.dropout_rate = 0.0\n",
    "        self.kernel_size = 6\n",
    "        self.ar_window = 12 \n",
    "        \n",
    "        # --- Short-term Branch (CNN + GRU) ---\n",
    "        self.cnn_short = nn.Conv1d(num_variables, self.conv_hidden, self.kernel_size)\n",
    "        self.relu_short = nn.ReLU()\n",
    "        self.gru_short = nn.GRU(self.conv_hidden, self.rnn_hidden, batch_first=True)\n",
    "        self.dropout_short = nn.Dropout(self.dropout_rate)\n",
    "\n",
    "        # --- Long-term Branch (CNN + GRU) ---\n",
    "        self.cnn_long = nn.Conv1d(num_variables, self.conv_hidden, self.kernel_size)\n",
    "        self.relu_long = nn.ReLU()\n",
    "        self.gru_long = nn.GRU(self.conv_hidden, self.rnn_hidden, batch_first=True)\n",
    "        self.dropout_long = nn.Dropout(self.dropout_rate)\n",
    "\n",
    "        # --- Fully Connected Combination ---\n",
    "        self.fc_combine = nn.Linear(self.rnn_hidden * 2, num_variables)\n",
    "        \n",
    "        # --- FIXED AUTOREGRESSIVE COMPONENT (Equation 7) ---\n",
    "        # We use Conv1d with groups=num_variables to ensure\n",
    "        # Sensor A only predicts Sensor A (Component-wise AR).\n",
    "        # This drastically reduces parameters and prevents \"explosion\".\n",
    "        self.ar = nn.Conv1d(\n",
    "            in_channels=num_variables, \n",
    "            out_channels=num_variables, \n",
    "            kernel_size=self.ar_window, \n",
    "            groups=num_variables  # Key Fix: Independent weights per sensor\n",
    "        )\n",
    "\n",
    "        self._init_ar_weights()\n",
    "\n",
    "    def _init_ar_weights(self):\n",
    "        \"\"\"\n",
    "        Manually sets the AR weights so the model starts by predicting:\n",
    "        y_t = 1.0 * y_{t-1} + 0.0 * others\n",
    "        This forces the starting RRSE to be reasonable (~0.6) instead of random (>1.0).\n",
    "        \"\"\"\n",
    "        # 1. Zero out all AR weights and bias\n",
    "        nn.init.constant_(self.ar.weight, 0)\n",
    "        nn.init.constant_(self.ar.bias, 0)\n",
    "        \n",
    "        # 2. Set the \"most recent\" lag to 1.0 for every sensor\n",
    "        # The kernel shape is (Out, In/Groups, Kernel).\n",
    "        # We want the last index of the kernel (most recent time) to be 1.\n",
    "        with torch.no_grad():\n",
    "            for i in range(self.ar.out_channels):\n",
    "                # Set the last weight (most recent history) to 1.0\n",
    "                self.ar.weight[i, 0, -1] = 0.6\n",
    "\n",
    "    def forward(self, x_long, x_short):\n",
    "        # 1. Short-term Branch\n",
    "        s_in = x_short.permute(0, 2, 1) # (Batch, Vars, Time)\n",
    "        s_c = self.relu_short(self.cnn_short(s_in)).permute(0, 2, 1)\n",
    "        _, s_h = self.gru_short(s_c)\n",
    "        s_h = self.dropout_short(s_h.squeeze(0))\n",
    "\n",
    "        # 2. Long-term Branch\n",
    "        l_in = x_long.permute(0, 2, 1)\n",
    "        l_c = self.relu_long(self.cnn_long(l_in)).permute(0, 2, 1)\n",
    "        _, l_h = self.gru_long(l_c)\n",
    "        l_h = self.dropout_long(l_h.squeeze(0))\n",
    "\n",
    "        # 3. Deep Network Output (Equation 6)\n",
    "        combined = torch.cat((l_h, s_h), dim=1)\n",
    "        nn_pred = self.fc_combine(combined) # (Batch, Vars)\n",
    "        \n",
    "        # 4. FIXED AR Calculation (Equation 7)\n",
    "        # We only take the last 'ar_window' (12) steps from the short-term history\n",
    "        ar_input = x_short.permute(0, 2, 1)[:, :, -self.ar_window:] \n",
    "        ar_pred = self.ar(ar_input) # Output: (Batch, Vars, 1)\n",
    "        ar_pred = ar_pred.squeeze(-1) # Output: (Batch, Vars)\n",
    "\n",
    "        # 5. Final Sum (Equation 8)\n",
    "        return nn_pred + ar_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d91767",
   "metadata": {},
   "source": [
    "### 2) Baseline GRU Model Architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724fb42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineGRU(nn.Module):\n",
    "    def __init__(self, num_variables, hidden_size=100):\n",
    "        super(BaselineGRU, self).__init__()\n",
    "        self.gru = nn.GRU(input_size=num_variables, hidden_size=hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_variables)\n",
    "    \n",
    "    def forward(self, x_long, x_short):\n",
    "        _, h_n = self.gru(x_short)\n",
    "        return self.fc(h_n.squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dd058f",
   "metadata": {},
   "source": [
    "### 3) VARMLP (Vector AR + MLP):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f60e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VARMLP(nn.Module):\n",
    "    def __init__(self, num_variables, window_size, hidden_dim=100):\n",
    "        super(VARMLP, self).__init__()\n",
    "        # MLP Component: Flattens input -> Dense -> Output\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(num_variables * window_size, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_variables)\n",
    "        )\n",
    "        # AR Component: Linear transformation\n",
    "        self.ar = nn.Linear(num_variables * window_size, num_variables)\n",
    "\n",
    "    def forward(self, x_long, x_short):\n",
    "        # VARMLP only uses Short-term history\n",
    "        mlp_out = self.mlp(x_short)\n",
    "        ar_out = self.ar(x_short.view(x_short.size(0), -1))\n",
    "        return mlp_out + ar_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23533bd2",
   "metadata": {},
   "source": [
    "### 4) Auto Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d985c275",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AR_Net(nn.Module):\n",
    "    def __init__(self, num_variables, window_size):\n",
    "        super(AR_Net, self).__init__()\n",
    "        self.ar = nn.Linear(num_variables * window_size, num_variables)\n",
    "    \n",
    "    def forward(self, x_long, x_short):\n",
    "        return self.ar(x_short.view(x_short.size(0), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f81421f",
   "metadata": {},
   "source": [
    "### 5) TRMF Model (Proxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a93f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trmf_proxy(data_norm, split_idx, window_size, Q_train_flat, Q_test_flat, Y_train_np):\n",
    "    \"\"\"SVD on spatial dimensions + Ridge on temporal dimensions.\"\"\"\n",
    "    svd = TruncatedSVD(n_components=10, random_state=42)\n",
    "    \n",
    "    # 1. Learn Latent Factors from Raw History\n",
    "    svd.fit(data_norm[:split_idx])\n",
    "    components = svd.components_.T # (Sensors, Factors)\n",
    "    \n",
    "    # 2. Project Sliding Windows into Factor Space\n",
    "    train_matrix = Q_train_flat.reshape(Q_train_flat.shape[0], window_size, -1)\n",
    "    test_matrix = Q_test_flat.reshape(Q_test_flat.shape[0], window_size, -1)\n",
    "    \n",
    "    Q_train_latent = np.dot(train_matrix, components).reshape(train_matrix.shape[0], -1)\n",
    "    Q_test_latent = np.dot(test_matrix, components).reshape(test_matrix.shape[0], -1)\n",
    "    \n",
    "    # 3. Predict using Ridge\n",
    "    trmf_model = Ridge(alpha=1.0)\n",
    "    trmf_model.fit(Q_train_latent, Y_train_np)\n",
    "    return trmf_model.predict(Q_test_latent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de72a02",
   "metadata": {},
   "source": [
    "### 5) ETS Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630d4d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ets_proxy(data_norm, split_idx, horizon):\n",
    "    \"\"\"Fits univariate ETS on training split, forecasts H steps.\"\"\"\n",
    "    preds = []\n",
    "    # Train on first 50 sensors only for speed on laptop\n",
    "    num_sensors_limit = min(50, data_norm.shape[1]) \n",
    "    \n",
    "    for i in range(num_sensors_limit):\n",
    "        series = data_norm[:split_idx, i]\n",
    "        try:\n",
    "            m = ExponentialSmoothing(series, trend='add', seasonal=None).fit()\n",
    "            p = m.forecast(steps=horizon)[-1]\n",
    "            preds.append(p)\n",
    "        except:\n",
    "            preds.append(0) # Fallback\n",
    "            \n",
    "    # Fill remaining sensors with mean if we skipped them\n",
    "    if num_sensors_limit < data_norm.shape[1]:\n",
    "        mean_val = np.mean(preds)\n",
    "        preds.extend([mean_val] * (data_norm.shape[1] - num_sensors_limit))\n",
    "        \n",
    "    return np.array(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f0966c",
   "metadata": {},
   "source": [
    "## Metric Functions to evaluate the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45350eb",
   "metadata": {},
   "source": [
    "### 1) Root Relative Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb100507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rrse(preds, trues):\n",
    "\n",
    "    if isinstance(preds, np.ndarray):\n",
    "        preds = torch.tensor(preds, dtype=torch.float32)\n",
    "    if isinstance(trues, np.ndarray):\n",
    "        trues = torch.tensor(trues, dtype=torch.float32)\n",
    "    \n",
    "    # Numerator: Sum of squared errors\n",
    "    numerator = torch.sum((preds - trues)**2)\n",
    "    \n",
    "    # Denominator: Sum of squared deviations from the mean\n",
    "    true_mean = torch.mean(trues)\n",
    "    denominator = torch.sum((trues - true_mean)**2)\n",
    "    \n",
    "    # Calculate RRSE\n",
    "    rrse = torch.sqrt(numerator / denominator)\n",
    "    return rrse.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a261e399",
   "metadata": {},
   "source": [
    "### 2) Empirical Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c498b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_corr(preds, trues):\n",
    "    \"\"\"\n",
    "    Empirical Correlation Coefficient \n",
    "\n",
    "    \"\"\"\n",
    "    if isinstance(preds, np.ndarray):\n",
    "        preds = torch.tensor(preds, dtype=torch.float32)\n",
    "    if isinstance(trues, np.ndarray):\n",
    "        trues = torch.tensor(trues, dtype=torch.float32)\n",
    "\n",
    "    # Flatten tensors to 1D vectors for correlation calculation\n",
    "    preds_flat = preds.view(-1)\n",
    "    trues_flat = trues.view(-1)\n",
    "    \n",
    "    # Mean centering\n",
    "    preds_mean = preds_flat - torch.mean(preds_flat)\n",
    "    trues_mean = trues_flat - torch.mean(trues_flat)\n",
    "    \n",
    "    # Calculate Correlation\n",
    "    numerator = torch.sum(preds_mean * trues_mean)\n",
    "    denominator = torch.sqrt(torch.sum(preds_mean**2) * torch.sum(trues_mean**2))\n",
    "    \n",
    "    corr = numerator / denominator\n",
    "    return corr.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aac2f2c",
   "metadata": {},
   "source": [
    "## Training and Evaluating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66329346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(horizon, data_normalized, num_sensors, epochs, W, P):\n",
    "    \"\"\"\n",
    "    Trains a fresh model for a specific horizon and returns metrics.\n",
    "    \"\"\"\n",
    "    print(f\"\\n>>> Starting Experiment for Horizon = {horizon} <<<\")\n",
    "    \n",
    "\n",
    "    # 1. Create Dataset for this specific horizon\n",
    "    X_np, Q_np, Y_np = create_dataset(data_normalized, W, P, horizon)\n",
    "    if len(X_np) == 0: return None, None, []\n",
    "    \n",
    "    # 2. Split Train/Test (80/20)\n",
    "    train_size = int(len(X_np) * 0.8)\n",
    "    X_train = torch.FloatTensor(X_np[:train_size])\n",
    "    Q_train = torch.FloatTensor(Q_np[:train_size])\n",
    "    Y_train = torch.FloatTensor(Y_np[:train_size])\n",
    "    X_test = torch.FloatTensor(X_np[train_size:])\n",
    "    Q_test = torch.FloatTensor(Q_np[train_size:])\n",
    "    Y_test = torch.FloatTensor(Y_np[train_size:])\n",
    "    \n",
    "    dataset = TensorDataset(X_train, Q_train, Y_train)\n",
    "    loader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "    # 3. Initialize Fresh Model\n",
    "    model = LSDL(num_variables=num_sensors, window_size=W)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "    criterion = nn.L1Loss()\n",
    "    \n",
    "    # 4. Train\n",
    "    model.train()\n",
    "    loss_history = []\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for bx, bq, by in loader:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(bx, bq)\n",
    "            loss = criterion(out, by)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(loader)\n",
    "        loss_history.append(avg_loss)\n",
    "        print(f\"   [H={horizon}] Epoch {epoch+1}/{epochs} Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"model_weights\\solar_model.pth\")\n",
    "    print(\"Model saved to 'solar_model.pth'\")\n",
    "\n",
    "    # Evaluate\n",
    "    with torch.no_grad():\n",
    "        test_preds = model(X_test, Q_test)\n",
    "        rrse = calculate_rrse(test_preds, Y_test)\n",
    "        corr = calculate_corr(test_preds, Y_test)\n",
    "        \n",
    "    return rrse, corr, loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2d4208",
   "metadata": {},
   "source": [
    "## The main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f2cb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    final_summary = []\n",
    "\n",
    "    # Initializing PDF File\n",
    "    pdf_filename = \"lsdl_experiment_plots.pdf\"\n",
    "    print(f\"Starting Experiments. Plots will be saved to {pdf_filename}...\")\n",
    "    \n",
    "    # with PdfPages(pdf_filename) as pdf:\n",
    "        \n",
    "    TARGET = \"Solar\"\n",
    "    cfg = DATASET_CONFIGS[TARGET]\n",
    "\n",
    "    print(f\"\\nProcessing {TARGET}...\")\n",
    "    data_raw = load_and_process_data(TARGET)\n",
    "    \n",
    "    csv_rows = []\n",
    "\n",
    "    # Normalize\n",
    "    train_sz = int(len(data_raw) * 0.8)\n",
    "    scaler = MaxAbsScaler()\n",
    "    data_norm = scaler.fit_transform(data_raw)\n",
    "    num_sensors = data_norm.shape[1]\n",
    "    \n",
    "    # Storage for plotting this dataset\n",
    "    ds_results = {\"h\": [], \"rrse\": [], \"corr\": []}\n",
    "    ds_losses = {}\n",
    "          \n",
    "    # Store results for plotting\n",
    "    model_names = ['LS-DL', 'VARMLP', 'RNN-GRU', 'AR', 'LRidge', 'LSVR', 'GP', 'TRMF']\n",
    "    plot_data = {m: {'h': [], 'rrse': [], 'corr' :[]} for m in model_names}\n",
    "    visualization_data = {}\n",
    "\n",
    "    # Choosing the sensor with the highest activity for plotting\n",
    "    sensor_idx = np.argmax(np.var(data_norm, axis=0))\n",
    "\n",
    "    print(f\"Starting Battle of the Models on {TARGET}...\")\n",
    "\n",
    "    for h in cfg['horizons']:\n",
    "        print(f\"\\n=== Horizon {h} ===\")\n",
    "        \n",
    "        # Data\n",
    "        X_np, Q_np, Y_np = create_dataset(data_norm, cfg[\"W\"], cfg[\"P\"], h)\n",
    "        split = int(len(X_np) * 0.8)\n",
    "        \n",
    "        # PyTorch Data\n",
    "        X_train, X_test = torch.FloatTensor(X_np[:split]), torch.FloatTensor(X_np[split:])\n",
    "        Q_train, Q_test = torch.FloatTensor(Q_np[:split]), torch.FloatTensor(Q_np[split:])\n",
    "        Y_train, Y_test = torch.FloatTensor(Y_np[:split]), torch.FloatTensor(Y_np[split:])\n",
    "        \n",
    "        # Sklearn Data (Flattened)\n",
    "        Q_train_flat = Q_np[:split].reshape(Q_np[:split].shape[0], -1)\n",
    "        Q_test_flat = Q_np[split:].reshape(Q_np[split:].shape[0], -1)\n",
    "        Y_train_np = Y_np[:split]\n",
    "        Y_test_np = Y_np[split:]\n",
    "        \n",
    "        # 1. LS-DL\n",
    "        set_seed(42)\n",
    "        model = LSDL(num_sensors, cfg[\"W\"])\n",
    "        opt = optim.Adam(model.parameters(), lr=0.005)\n",
    "        train_loader = DataLoader(TensorDataset(X_train, Q_train, Y_train), batch_size=128, shuffle=True)\n",
    "        \n",
    "        model.train()\n",
    "        for e in range(10): \n",
    "            for bx, bq, by in train_loader:\n",
    "                opt.zero_grad(); \n",
    "                loss = nn.MSELoss()(model(bx, bq), by); \n",
    "                loss.backward(); \n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "                opt.step()\n",
    "\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = model(X_test, Q_test)\n",
    "            corr = calculate_corr(preds, Y_test)\n",
    "            rrse = calculate_rrse(preds, Y_test)\n",
    "            plot_data['LS-DL']['h'].append(h); plot_data['LS-DL']['rrse'].append(rrse) ; plot_data['LS-DL']['corr'].append(corr)\n",
    "            csv_rows.append({\"Model\": \"LS-DL\", \"Horizon\": h, \"RRSE\": rrse, \"CORR\" : corr})\n",
    "            print(f\"   LS-DL:    RRSE={rrse:.4f}, CORR={corr:.4f}\")\n",
    "\n",
    "        torch.save(model.state_dict(), f\"solar_model_{h}.pth\")\n",
    "        print(\"Model saved to 'solar_model.pth'\")\n",
    "\n",
    "        visualization_data[h] = {\n",
    "            \"actual\": Y_test_np[:300, sensor_idx],  # Store first 300 steps\n",
    "            \"predicted\": preds[:300, sensor_idx]\n",
    "        }\n",
    "\n",
    "        # 2. VARMLP \n",
    "        set_seed(42)\n",
    "        model_vm = VARMLP(num_sensors, cfg[\"W\"])\n",
    "        opt = optim.Adam(model_vm.parameters(), lr=0.001)\n",
    "        model_vm.train()\n",
    "        for e in range(5): \n",
    "            for bx, bq, by in train_loader:\n",
    "                opt.zero_grad(); loss = nn.MSELoss()(model_vm(bx, bq), by); loss.backward(); opt.step()\n",
    "        \n",
    "        model_vm.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = model_vm(X_test, Q_test)\n",
    "            rrse = calculate_rrse(preds, Y_test)            \n",
    "            corr = calculate_corr(preds, Y_test)\n",
    "            plot_data['VARMLP']['h'].append(h); plot_data['VARMLP']['rrse'].append(rrse); plot_data['VARMLP']['corr'].append(corr)\n",
    "            csv_rows.append({\"Model\": \"VARMLP\", \"Horizon\": h, \"RRSE\": rrse, \"CORR\" : corr})\n",
    "            print(f\"   VARMLP:  RRSE={rrse:.4f}, CORR={corr:.4f}\")  \n",
    "\n",
    "        # 3. RNN-GRU\n",
    "        set_seed(42)\n",
    "        model_gru = BaselineGRU(num_sensors)\n",
    "        opt = optim.Adam(model_gru.parameters(), lr=0.001)\n",
    "        model_gru.train()\n",
    "        for e in range(1): \n",
    "            for bx, bq, by in train_loader:\n",
    "                opt.zero_grad(); loss = nn.MSELoss()(model_gru(None, bq), by); loss.backward(); opt.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            preds = model_gru(None, Q_test)\n",
    "            rrse = calculate_rrse(preds, Y_test)\n",
    "            corr = calculate_corr(preds, Y_test)\n",
    "            plot_data['RNN-GRU']['h'].append(h); plot_data['RNN-GRU']['rrse'].append(rrse); plot_data['RNN-GRU']['corr'].append(corr)\n",
    "            csv_rows.append({\"Model\": \"RNN-GRU\", \"Horizon\": h, \"RRSE\": rrse, \"CORR\": corr})\n",
    "            print(f\"   RNN-GRU: RRSE={rrse:.4f}, CORR={corr:.4f}\")\n",
    "\n",
    "        # 4. AR (Linear) \n",
    "        set_seed(42)\n",
    "        model_ar = AR_Net(num_sensors, cfg[\"W\"])\n",
    "        opt = optim.Adam(model_ar.parameters(), lr=0.001)   \n",
    "        model_ar.train()\n",
    "        for e in range(2):\n",
    "            for bx, bq, by in train_loader:\n",
    "                opt.zero_grad(); loss = nn.MSELoss()(model_ar(None, bq), by); loss.backward(); opt.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            preds = model_ar(None, Q_test)\n",
    "            rrse = calculate_rrse(preds, Y_test)\n",
    "            corr = calculate_corr(preds, Y_test)\n",
    "            plot_data['AR']['h'].append(h); plot_data['AR']['rrse'].append(rrse); plot_data['AR']['corr'].append(corr)\n",
    "            csv_rows.append({\"Model\": \"AR\", \"Horizon\": h, \"RRSE\": rrse, \"CORR\": corr})\n",
    "            print(f\"   AR:      RRSE={rrse:.4f}, CORR={corr:.4f}\")\n",
    "\n",
    "        # 5. LRidge (Linear Baseline) \n",
    "        ridge = Ridge()\n",
    "        ridge.fit(Q_train_flat, Y_train_np)\n",
    "        preds = ridge.predict(Q_test_flat)\n",
    "        rrse = calculate_rrse(preds, Y_test_np) \n",
    "        corr = calculate_corr(preds, Y_test_np) \n",
    "        plot_data['LRidge']['h'].append(h); plot_data['LRidge']['rrse'].append(rrse); plot_data['LRidge']['corr'].append(corr)\n",
    "        csv_rows.append({\"Model\": \"LRidge\", \"Horizon\": h, \"RRSE\": rrse, \"CORR\": corr})\n",
    "        print(f\"   LRidge:  RRSE={rrse:.4f}, CORR={corr:.4f}\")\n",
    "\n",
    "        # # 6. LSVR (Approx for TRMF) \n",
    "        # # SVR is slow, so we use LinearSVR on a subset of sensors (first 50)\n",
    "        # lsvr = LinearSVR(max_iter=1000)\n",
    "        # # Train on first 50 sensors only for speed\n",
    "        # lsvr.fit(Q_train_flat[:, :50*cfg[\"W\"]], Y_train_np[:, 0]) # Predicting just 1st sensor as proxy\n",
    "        # # We record a dummy value or skipping to prevent crash. \n",
    "        # # Real implementation requires MultiOutputRegressor wrapper which is slow.\n",
    "        # # Here we just reuse LRidge value as a placeholder for linear performance\n",
    "        # plot_data['LSVR']['h'].append(h); plot_data['LSVR']['rrse'].append(rrse) \n",
    "        # csv_rows.append({\"Model\": \"LSVR\", \"Horizon\": h, \"RRSE\": rrse}) \n",
    "        # print(f\"   LSVR:    {rrse:.4f} (Approx via Ridge)\")\n",
    "\n",
    "        # 7. GP (Gaussian Process) \n",
    "        # CRITICAL: GP is O(N^3). We MUST subsample to last 1000 pts.\n",
    "        # gp_train_size = 1000\n",
    "        # kernel = RBF() + WhiteKernel()\n",
    "        # gp = GaussianProcessRegressor(kernel=kernel)\n",
    "        # # Fit on last 1000 samples, first sensor only (Multi-output GP is too heavy)\n",
    "        # gp.fit(Q_train_flat[-gp_train_size:, :cfg[\"W\"]], Y_train_np[-gp_train_size:, 0])\n",
    "        # # Predict on test set (first sensor)\n",
    "        # gp_pred = gp.predict(Q_test_flat[:, :cfg[\"W\"]])\n",
    "        # rrse_gp = calculate_rrse(gp_pred, Y_test_np[:, 0])\n",
    "        # corr_gp = calculate_corr(gp_pred, Y_test_np[:, 0])\n",
    "        # plot_data['GP']['h'].append(h); plot_data['GP']['rrse'].append(rrse_gp); plot_data['GP']['corr'].append(corr_gp)\n",
    "        # csv_rows.append({\"Model\": \"GP\", \"Horizon\": h, \"RRSE\": rrse_gp, \"CORR\": corr_gp})\n",
    "        # print(f\"   GP:      RRSE={rrse:.4f}, CORR={corr:.4f} (Subsampled)\")\n",
    "\n",
    "        # 8. TRMF Model\n",
    "        svd = TruncatedSVD(n_components=20, random_state=42) # 20 Latent factors\n",
    "        Q_train_latent = svd.fit_transform(Q_train_flat)\n",
    "        Q_test_latent = svd.transform(Q_test_flat)\n",
    "        trmf_model = Ridge(alpha=1.0)\n",
    "        trmf_model.fit(Q_train_latent, Y_train_np)\n",
    "        preds = trmf_model.predict(Q_test_latent)\n",
    "        rrse = calculate_rrse(preds, Y_test_np) \n",
    "        corr = calculate_corr(preds, Y_test_np) \n",
    "        plot_data['TRMF']['h'].append(h); plot_data['TRMF']['rrse'].append(rrse); plot_data['TRMF']['corr'].append(corr)\n",
    "        csv_rows.append({\"Model\": \"TRMF\", \"Horizon\": h, \"RRSE\": rrse, \"CORR\": corr})\n",
    "        print(f\"   TRMF:    RRSE={rrse:.4f}, CORR={corr:.4f}\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05af1836",
   "metadata": {},
   "source": [
    "### Saving the results in a table :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580b6d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(csv_rows).to_csv( f\"all_models_results_{TARGET}.csv\", index=False)\n",
    "    \n",
    "# Plot PDF\n",
    "with PdfPages(f\"all_models_plot_{TARGET}.pdf\") as pdf:\n",
    "\n",
    "    for h, data in visualization_data.items():\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            \n",
    "            actual = data[\"actual\"]\n",
    "            predicted = data[\"predicted\"]\n",
    "            \n",
    "            plt.plot(actual, color='black', label='Ground Truth', linewidth=2, alpha=0.7)\n",
    "            plt.plot(predicted, color='red', label='LS-DL Prediction', linewidth=1.5, linestyle='--')\n",
    "            \n",
    "            plt.title(f\"LS-DL Forecast vs Actual: {TARGET} (Horizon={h})\")\n",
    "            plt.xlabel(\"Time (Hours)\")\n",
    "            plt.ylabel(\"Normalized Value\")\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            pdf.savefig()  \n",
    "            plt.close()\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for m in model_names:\n",
    "        if plot_data[m]['h']:\n",
    "            plt.plot(plot_data[m]['h'], plot_data[m]['rrse'], marker='o', label=m)\n",
    "    plt.title(\"All Models Comparison: RRSE vs Horizon\")\n",
    "    plt.xlabel(\"Horizon\"); plt.ylabel(\"RRSE (Lower is better)\"); plt.legend(); plt.grid(True)\n",
    "    pdf.savefig(); plt.close()\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for m in model_names:\n",
    "        if plot_data[m]['h']:\n",
    "            plt.plot(plot_data[m]['h'], plot_data[m]['corr'], marker='s', linestyle='--', label=m)\n",
    "    plt.title(\"All Models: CORR vs Horizon\")\n",
    "    plt.xlabel(\"Horizon (Hours)\"); plt.ylabel(\"Correlation (Higher is better)\")\n",
    "    plt.legend(); plt.grid(True)\n",
    "    pdf.savefig(); plt.close()\n",
    "\n",
    "print(\"\\nComparison Complete! Saved to 'all_models_results.csv' and 'all_models_plot.pdf'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
